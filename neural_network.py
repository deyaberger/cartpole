# -*- coding: utf-8 -*-
"""neural_network.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1EBUOx48jxvo-C5G0AOLlF5Valj68FMZx
"""

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers

import gym
from matplotlib import pyplot as plt
from config import infos
import pickle
from collections import deque
import numpy as np
import random

env = gym.envs.make("CartPole-v1")
state_size = env.observation_space.shape[0]
action_size = env.action_space.n
output_dir = "./cartpole/outs"
memory = deque(maxlen=2000)

def init_model(state_size, action_size):
    learning_rate = infos.learning_rate
    model = keras.Sequential()
    model.add(keras.layers.Dense(24, input_shape=[state_size], activation='relu'))
    model.add(keras.layers.Dense(12, activation='relu'))
    model.add(keras.layers.Dense(action_size, activation='linear'))
    model.compile(loss='mse', optimizer=tf.keras.optimizers.Adam(lr=learning_rate))
    return model

def policy(state, predicted_qvalues):
    if (random.random() < infos.epsilon):
        action = random.randint(0, 1)
    else:
        action = np.argmax(predicted_qvalues)
    return (action)

def load(name, model):
    model.load_weights(name)
    
def save(name, model):
    model.save_weights(name)

def fit_model(state, action, reward, new_state, m1, m2, target_qvalues):
    target = reward + (infos.discount_factor * max(m2.predict(new_state)[0]))
    target_qvalues[0][action] = target 
    m1.fit(state, target_qvalues, epochs=1, verbose=0)
    return m1, m2

def learn():
    m1 = init_model(state_size, action_size)
    m2 = keras.models.clone_model(m1)
    for episode in range(infos.episodes):
        state = env.reset()
        state = np.reshape(state, [1, state_size])
        steps = 0
        done = False
        
        while not done:
            predicted_qvalues = m1.predict(state)
            action = policy(state, predicted_qvalues[0])
            new_state, reward, done, _  = env.step(action)
            new_state = np.reshape(new_state, [1, state_size])
            steps += 1
            if done == True:
                reward = infos.reward_values[0]
            memory.append((state, action, reward, new_state, done))
            m1, m2 = fit_model(state, action, reward, new_state, m1, m2, predicted_qvalues)
            state = new_state
        
        if len(memory) > 200 and (random.random() < 0.5):
          print(f"*** memory replay for episode:{episode}")
          minibatch = random.sample(memory, infos.batch_size)
          for state, action, reward, new_state, done in minibatch:
              predicted_qvalues = m1.predict(state)
              action = policy(state, predicted_qvalues[0])
              m1, m2 = fit_model(state, action, reward, new_state, m1, m2, predicted_qvalues)

        infos.epsilon = infos.epsilon * infos.epsilon_decay
        if (infos.epsilon < infos.epislon_min):
            infos.epsilon = 0.99
            
        print(f'\nepisode = {episode}, total_steps = {steps} and epsilon == {round(infos.epsilon, 3)}')
        m2 = keras.models.clone_model(m1)
            
        if episode % 50 == 0 and episode != 0:
            save(f'outs/with_dqn_{episode}.hdf5', m1)

learn()

